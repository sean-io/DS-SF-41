{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Demo\n",
    "\n",
    "If you haven't installed spacy yet, use:\n",
    "```\n",
    "conda install spacy\n",
    "python -m spacy.en.download\n",
    "```\n",
    "This downloads about 500 MB of data.\n",
    "\n",
    "Another popular package, `nltk`, can be installed as follows (you can skip this for now):\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "python -m nltk.downloader all\n",
    "```\n",
    "\n",
    "This also downloads a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load StumbleUpon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>urlid</th>\n",
       "      <th>boilerplate</th>\n",
       "      <th>alchemy_category</th>\n",
       "      <th>alchemy_category_score</th>\n",
       "      <th>avglinksize</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>commonlinkratio_2</th>\n",
       "      <th>commonlinkratio_3</th>\n",
       "      <th>commonlinkratio_4</th>\n",
       "      <th>...</th>\n",
       "      <th>linkwordscore</th>\n",
       "      <th>news_front_page</th>\n",
       "      <th>non_markup_alphanum_characters</th>\n",
       "      <th>numberOfLinks</th>\n",
       "      <th>numwords_in_url</th>\n",
       "      <th>parametrizedLinkRatio</th>\n",
       "      <th>spelling_errors_ratio</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bloomberg.com/news/2010-12-23/ibm-p...</td>\n",
       "      <td>4042</td>\n",
       "      <td>{\"title\":\"IBM Sees Holographic Calls Air Breat...</td>\n",
       "      <td>business</td>\n",
       "      <td>0.789131</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>5424</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.079130</td>\n",
       "      <td>0</td>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batte...</td>\n",
       "      <td>A sign stands outside the International Busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.popsci.com/technology/article/2012-...</td>\n",
       "      <td>8471</td>\n",
       "      <td>{\"title\":\"The Fully Electronic Futuristic Star...</td>\n",
       "      <td>recreation</td>\n",
       "      <td>0.574147</td>\n",
       "      <td>3.677966</td>\n",
       "      <td>0.508021</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4973</td>\n",
       "      <td>187</td>\n",
       "      <td>9</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.125448</td>\n",
       "      <td>1</td>\n",
       "      <td>The Fully Electronic Futuristic Starting Gun T...</td>\n",
       "      <td>And that can be carried on a plane without the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.menshealth.com/health/flu-fighting-...</td>\n",
       "      <td>1164</td>\n",
       "      <td>{\"title\":\"Fruits that Fight the Flu fruits tha...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>2.382883</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.321705</td>\n",
       "      <td>0.120155</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2240</td>\n",
       "      <td>258</td>\n",
       "      <td>11</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>1</td>\n",
       "      <td>Fruits that Fight the Flu fruits that fight th...</td>\n",
       "      <td>Apples The most popular source of antioxidants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.dumblittleman.com/2007/12/10-foolpr...</td>\n",
       "      <td>6684</td>\n",
       "      <td>{\"title\":\"10 Foolproof Tips for Better Sleep \"...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.801248</td>\n",
       "      <td>1.543103</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2737</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.100858</td>\n",
       "      <td>1</td>\n",
       "      <td>10 Foolproof Tips for Better Sleep</td>\n",
       "      <td>There was a period in my life when I had a lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://bleacherreport.com/articles/1205138-the...</td>\n",
       "      <td>9006</td>\n",
       "      <td>{\"title\":\"The 50 Coolest Jerseys You Didn t Kn...</td>\n",
       "      <td>sports</td>\n",
       "      <td>0.719157</td>\n",
       "      <td>2.676471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12032</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0</td>\n",
       "      <td>The 50 Coolest Jerseys You Didn t Know Existed...</td>\n",
       "      <td>Jersey sales is a curious business Whether you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  urlid  \\\n",
       "0  http://www.bloomberg.com/news/2010-12-23/ibm-p...   4042   \n",
       "1  http://www.popsci.com/technology/article/2012-...   8471   \n",
       "2  http://www.menshealth.com/health/flu-fighting-...   1164   \n",
       "3  http://www.dumblittleman.com/2007/12/10-foolpr...   6684   \n",
       "4  http://bleacherreport.com/articles/1205138-the...   9006   \n",
       "\n",
       "                                         boilerplate alchemy_category  \\\n",
       "0  {\"title\":\"IBM Sees Holographic Calls Air Breat...         business   \n",
       "1  {\"title\":\"The Fully Electronic Futuristic Star...       recreation   \n",
       "2  {\"title\":\"Fruits that Fight the Flu fruits tha...           health   \n",
       "3  {\"title\":\"10 Foolproof Tips for Better Sleep \"...           health   \n",
       "4  {\"title\":\"The 50 Coolest Jerseys You Didn t Kn...           sports   \n",
       "\n",
       "  alchemy_category_score  avglinksize  commonlinkratio_1  commonlinkratio_2  \\\n",
       "0               0.789131     2.055556           0.676471           0.205882   \n",
       "1               0.574147     3.677966           0.508021           0.288770   \n",
       "2               0.996526     2.382883           0.562016           0.321705   \n",
       "3               0.801248     1.543103           0.400000           0.100000   \n",
       "4               0.719157     2.676471           0.500000           0.222222   \n",
       "\n",
       "   commonlinkratio_3  commonlinkratio_4  \\\n",
       "0           0.047059           0.023529   \n",
       "1           0.213904           0.144385   \n",
       "2           0.120155           0.042636   \n",
       "3           0.016667           0.000000   \n",
       "4           0.123457           0.043210   \n",
       "\n",
       "                         ...                          linkwordscore  \\\n",
       "0                        ...                                     24   \n",
       "1                        ...                                     40   \n",
       "2                        ...                                     55   \n",
       "3                        ...                                     24   \n",
       "4                        ...                                     14   \n",
       "\n",
       "   news_front_page  non_markup_alphanum_characters  numberOfLinks  \\\n",
       "0                0                            5424            170   \n",
       "1                0                            4973            187   \n",
       "2                0                            2240            258   \n",
       "3                0                            2737            120   \n",
       "4                0                           12032            162   \n",
       "\n",
       "   numwords_in_url  parametrizedLinkRatio  spelling_errors_ratio label  \\\n",
       "0                8               0.152941               0.079130     0   \n",
       "1                9               0.181818               0.125448     1   \n",
       "2               11               0.166667               0.057613     1   \n",
       "3                5               0.041667               0.100858     1   \n",
       "4               10               0.098765               0.082569     0   \n",
       "\n",
       "                                               title  \\\n",
       "0  IBM Sees Holographic Calls Air Breathing Batte...   \n",
       "1  The Fully Electronic Futuristic Starting Gun T...   \n",
       "2  Fruits that Fight the Flu fruits that fight th...   \n",
       "3                10 Foolproof Tips for Better Sleep    \n",
       "4  The 50 Coolest Jerseys You Didn t Know Existed...   \n",
       "\n",
       "                                                body  \n",
       "0  A sign stands outside the International Busine...  \n",
       "1  And that can be carried on a plane without the...  \n",
       "2  Apples The most popular source of antioxidants...  \n",
       "3  There was a period in my life when I had a lot...  \n",
       "4  Jersey sales is a curious business Whether you...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('../../data/stumbleupon.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "df['title'] = df['boilerplate'].map(lambda x: json.loads(x).get('title', ''))\n",
    "df['body'] = df['boilerplate'].map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: IBM\n",
      "\t Phrase type: nsubj\n",
      "\t Is the word a known entity type? ORG\n",
      "\t Lemma: ibm\n",
      "\t Parent of this word: see\n",
      "Word: Sees\n",
      "\t Phrase type: compound\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: see\n",
      "\t Parent of this word: calls\n",
      "Word: Holographic\n",
      "\t Phrase type: compound\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: holographic\n",
      "\t Parent of this word: calls\n",
      "Word: Calls\n",
      "\t Phrase type: ROOT\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: calls\n",
      "\t Parent of this word: calls\n",
      "Word: ,\n",
      "\t Phrase type: punct\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: ,\n",
      "\t Parent of this word: calls\n",
      "Word: Air\n",
      "\t Phrase type: compound\n",
      "\t Is the word a known entity type? ORG\n",
      "\t Lemma: air\n",
      "\t Parent of this word: battery\n",
      "Word: Breathing\n",
      "\t Phrase type: compound\n",
      "\t Is the word a known entity type? ORG\n",
      "\t Lemma: breathing\n",
      "\t Parent of this word: battery\n",
      "Word: Battery\n",
      "\t Phrase type: appos\n",
      "\t Is the word a known entity type? ORG\n",
      "\t Lemma: battery\n",
      "\t Parent of this word: calls\n"
     ]
    }
   ],
   "source": [
    "# Try replacing this sentence with another sentence to see how it's parsed\n",
    "title = u\"IBM Sees Holographic Calls, Air Breathing Battery\"\n",
    "parsed = nlp(title)\n",
    "\n",
    "for (i, word) in enumerate(parsed): \n",
    "    print 'Word: {}'.format(word)\n",
    "    print '\\t Phrase type: {}'.format(word.dep_)\n",
    "    print '\\t Is the word a known entity type? {}'.format(word.ent_type_ if word.ent_type_ else \"No\")\n",
    "    print '\\t Lemma: {}'.format(word.lemma_)\n",
    "    print '\\t Parent of this word: {}'.format(word.head.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Page Titles\n",
    "\n",
    "Let's see if we can find organizations in our page titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Fully Electronic Futuristic Starting Gun T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Foolproof Tips for Better Sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fashion lane American Wild Child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Business Financial News Breaking US Internatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title\n",
       "0   IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1   The Fully Electronic Futuristic Starting Gun T...\n",
       "3                 10 Foolproof Tips for Better Sleep \n",
       "6                   fashion lane American Wild Child \n",
       "10  Business Financial News Breaking US Internatio..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def references_organization(title):\n",
    "    parsed = nlp(title)\n",
    "    return any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "\n",
    "df['references_organization'] = df['title'].fillna(u'').map(references_organization)\n",
    "\n",
    "# Take a look\n",
    "df[df['references_organization']][['title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The Fully Electronic Futuristic Starting Gun That Eliminates Advantages in Races the fully electronic, futuristic starting gun that eliminates advantages in races the fully electronic, futuristic starting gun that eliminates advantages in races'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['references_organization']]['title'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Exercise:\n",
    "\n",
    ">Write a function to identify titles that mention an organization (ORG) and a person (PERSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Fully Electronic Futuristic Starting Gun T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Foolproof Tips for Better Sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 50 Coolest Jerseys You Didn t Know Existed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fashion lane American Wild Child</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1  The Fully Electronic Futuristic Starting Gun T...\n",
       "3                10 Foolproof Tips for Better Sleep \n",
       "4  The 50 Coolest Jerseys You Didn t Know Existed...\n",
       "6                  fashion lane American Wild Child "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the code to this function\n",
    "def references_org_person(title):\n",
    "    parsed = nlp(title)\n",
    "    is_org = any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "    is_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n",
    "    return is_org or is_person\n",
    "\n",
    "df['references_org_person'] = df['title'].fillna(u'').map(references_org_person)\n",
    "\n",
    "# Take a look\n",
    "df[df['references_org_person']][['title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting \"Greenness\" Of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from [stumbleupon](https://www.stumbleupon.com/), a web page recommender.  \n",
    "\n",
    "A description of the columns is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FieldName|Type|Description\n",
    "---------|----|-----------\n",
    "url|string|Url of the webpage to be classified\n",
    "title|string|Title of the article\n",
    "body|string|Body text of article\n",
    "urlid|integer| StumbleUpon's unique identifier for each url\n",
    "boilerplate|json|Boilerplate text\n",
    "alchemy_category|string|Alchemy category (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "alchemy_category_score|double|Alchemy category score (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "avglinksize| double|Average number of words in each link\n",
    "commonlinkratio_1|double|# of links sharing at least 1 word with 1 other links / # of links\n",
    "commonlinkratio_2|double|# of links sharing at least 1 word with 2 other links / # of links\n",
    "commonlinkratio_3|double|# of links sharing at least 1 word with 3 other links / # of links\n",
    "commonlinkratio_4|double|# of links sharing at least 1 word with 4 other links / # of links\n",
    "compression_ratio|double|Compression achieved on this page via gzip (measure of redundancy)\n",
    "embed_ratio|double|Count of number of <embed> usage\n",
    "frameBased|integer (0 or 1)|A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "frameTagRatio|double|Ratio of iframe markups over total number of markups\n",
    "hasDomainLink|integer (0 or 1)|True (1) if it contains an <a> with an url with domain\n",
    "html_ratio|double|Ratio of tags vs text in the page\n",
    "image_ratio|double|Ratio of <img> tags vs text in the page\n",
    "is_news|integer (0 or 1) | True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "lengthyLinkDomain| integer (0 or 1)|True (1) if at least 3 <a> 's text contains more than 30 alphanumeric characters\n",
    "linkwordscore|double|Percentage of words on the page that are in hyperlink's text\n",
    "news_front_page| integer (0 or 1)|True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "non_markup_alphanum_characters|integer| Page's text's number of alphanumeric characters\n",
    "numberOfLinks|integer Number of <a>|markups\n",
    "numwords_in_url| double|Number of words in url\n",
    "parametrizedLinkRatio|double|A link is parametrized if it's url contains parameters or has an attached onClick event\n",
    "spelling_errors_ratio|double|Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "label|integer (0 or 1)|User-determined label. Either evergreen (1) or non-evergreen (0); available for train.tsv only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Let's try extracting some of the text content.\n",
    "> ### Create a feature for the title containing 'recipe'. Is the % of evegreen websites higher or lower on pages that have recipe in the the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_recipe\n",
       "False    0.455730\n",
       "True     0.912206\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['has_recipe'] = df['title'].str.lower().str.contains('recipe')\n",
    "df.groupby('has_recipe')['label'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Use of the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df['title'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1000, \n",
    "    ngram_range=(1, 2), \n",
    "    stop_words='english',\n",
    "    binary=True,\n",
    ")\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 715, Token: recipe\n",
      "Feature: 217, Token: com\n",
      "Feature: 721, Token: recipes\n",
      "Feature: 363, Token: food\n",
      "Feature: 192, Token: chocolate\n"
     ]
    }
   ],
   "source": [
    "# Let's find the most frequent words\n",
    "top_words = pd.DataFrame(X.toarray()).sum().sort_values(ascending=False).head()\n",
    "\n",
    "for word in top_words.index:\n",
    "    print 'Feature: {}, Token: {}'.format(word, vectorizer.get_feature_names()[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Build a random forest model to predict evergreeness of a website using the title features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.7888958   0.80497036  0.80298833], Average AUC 0.798951494351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=20)\n",
    "    \n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "# Note: we already fit the vectorizer so we don't need to repeat that step\n",
    "X = vectorizer.transform(titles).toarray()\n",
    "y = df['label']\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print 'CV AUC {}, Average AUC {}'.format(scores, scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75416667])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train a final model and apply it to a new article title\n",
    "model.fit(X, y)\n",
    "\n",
    "title = '4 Sweet Gifts for Your Valentine'\n",
    "title_as_features = vectorizer.transform([title])\n",
    "model.predict_proba(title_as_features)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a random forest model to predict evergreeness of a website using the title features and quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81502877777265004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "    'image_ratio', \n",
    "    'html_ratio',  \n",
    "    'avglinksize'\n",
    "]\n",
    "\n",
    "# Concatenate the quantitative features with the title features\n",
    "X = pd.concat([df[features], pd.DataFrame(vectorizer.transform(df['title'].fillna('')).toarray())], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "cross_val_score(model, X, y, scoring='roc_auc', cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Build a random forest model to predict evergreeness of a website using the body features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try this yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Use `TfIdfVectorizer` instead of `CountVectorizer` - is this an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000, \n",
    "    stop_words='english',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=u'english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = df['title'].fillna('')\n",
    "tfidf.fit(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 198, Token: com\n",
      "Feature: 721, Token: recipe\n",
      "Feature: 722, Token: recipes\n",
      "Feature: 356, Token: food\n",
      "Feature: 180, Token: chocolate\n"
     ]
    }
   ],
   "source": [
    "X = tfidf.transform(titles)\n",
    "\n",
    "# Let's find the highest weighted words\n",
    "top_words = pd.DataFrame(X.toarray()).sum().sort_values(ascending=False).head()\n",
    "\n",
    "for word in top_words.index:\n",
    "    print 'Feature: {}, Token: {}'.format(word, tfidf.get_feature_names()[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.80618878  0.8184249   0.81189199], Average AUC 0.812168558812\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print 'CV AUC {}, Average AUC {}'.format(scores, scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)\n",
    "\n",
    "# Try the model out on a new title\n",
    "title = '4 Sweet Gifts for Your Valentine'\n",
    "title_as_features = tfidf.transform([title])\n",
    "model.predict_proba(title_as_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
